# Planner Agent Test Suite

Comprehensive automated testing framework for the planner agent with 30 tests across 3 parallel sessions.

## Overview

This test suite validates the planner agent's ability to:
- **Classify intent** correctly (edit, act, clarify)
- **Enrich prompts** with detailed explanations
- **Manage context** with sliding window (last 2 prompts)
- **Summarize context** when length exceeds 100 characters
- **Generate unique step IDs** for each request
- **Maintain session state** across multiple requests

## Test Organization

### 30 Tests Across 3 Sessions

**Session A (10 tests)** - Core Functionality & Edge Cases
- Empty session ‚Üí First prompt (no context)
- Second prompt (single context)
- Third+ prompts (full 2-prompt context)
- Short context (<100 chars, no summarization)
- Long context (>100 chars, triggers summarization)
- Intent types: edit, act, clarify

**Session B (5 tests)** - Intent Classification Accuracy
- Unambiguous edit requests
- Clear action commands (scroll, click, navigate)
- Ambiguous queries requiring clarification

**Session C (15 tests)** - Stress Test & Variety
- Multiple consecutive edits
- Mixed edit/act sequences
- Context window boundary testing
- Step ID uniqueness verification
- Edge cases: very short/long prompts, special characters

## Running the Tests

### Prerequisites

1. **Unified server must be running:**

```bash
cd /Users/georgychomakhashvili/MBZUAI-Hackathon-DreamTeam
source llm/venv/bin/activate
python -m uvicorn llm.server:app --host 0.0.0.0 --port 8000
```

**Note:** Tests now use the unified server at `llm/server.py` which provides both `/decide` and `/clarify` endpoints.

2. **Dependencies installed:**

```bash
pip install httpx  # If not already installed
```

### Execute Tests

```bash
# From project root
cd /Users/georgychomakhashvili/MBZUAI-Hackathon-DreamTeam
./llm/venv/bin/python llm/planner/tests/runner.py
```

### Expected Output

```
============================================================
PLANNER AGENT TEST SUITE
============================================================

Checking server health...
‚úì Server is healthy

Running 30 tests across 3 sessions...
Sessions will run in parallel, tests within sessions run sequentially.

[session-a] Starting session tests...
[session-b] Starting session tests...
[session-c] Starting session tests...
...

============================================================
TEST RESULTS SUMMARY
============================================================
Total Tests:       30
Passed:            30 (100.0%)
Failed:            0
Classification:    93.3% accurate
Duration:          31.12s
============================================================

üéâ All tests passed!
```

## Test Results

Results are saved in `llm/planner/tests/results/` (gitignored) with **timestamped run folders**:

```
results/
‚îú‚îÄ‚îÄ run-1/                          # First test run
‚îÇ   ‚îú‚îÄ‚îÄ session-a.json
‚îÇ   ‚îú‚îÄ‚îÄ session-a_classification.json
‚îÇ   ‚îú‚îÄ‚îÄ session-b.json
‚îÇ   ‚îú‚îÄ‚îÄ session-b_classification.json
‚îÇ   ‚îú‚îÄ‚îÄ session-c.json
‚îÇ   ‚îú‚îÄ‚îÄ session-c_classification.json
‚îÇ   ‚îî‚îÄ‚îÄ summary.json
‚îú‚îÄ‚îÄ run-2/                          # Second test run
‚îÇ   ‚îú‚îÄ‚îÄ session-a.json
‚îÇ   ‚îú‚îÄ‚îÄ session-a_classification.json
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ run-3/                          # Third test run
    ‚îî‚îÄ‚îÄ ...
```

Each test run gets its own folder (`run-1`, `run-2`, etc.) so you can:
- Compare results across multiple runs
- Track improvements over time
- Keep historical test data
- Never overwrite previous results

### Result JSON Structure

```json
{
  "total_sessions": 3,
  "total_tests": 30,
  "passed": 30,
  "failed": 0,
  "success_rate": "100.0%",
  "classification_accuracy": "93.3%",
  "total_duration_seconds": 31.12,
  "sessions": [
    {
      "session_id": "session-a",
      "total_tests": 10,
      "passed": 10,
      "tests": [
        {
          "test_id": "A1",
          "name": "Empty session - first prompt",
          "input": {"sid": "session-a", "text": "..."},
          "response": {"step_id": "...", "step_type": "edit", ...},
          "assertions": {
            "has_step_id": true,
            "valid_step_type": true,
            "classification_correct": true,
            ...
          },
          "passed": true
        }
      ]
    }
  ]
}
```

## Validation Checks

### System Behavior
- ‚úì Response structure (step_id, step_type, intent, context)
- ‚úì Step ID format (valid UUID)
- ‚úì Step ID uniqueness across all tests
- ‚úì Step type validity (edit/act/clarify)
- ‚úì Intent format: `"user_text | explanation"`
- ‚úì Explanation quality (non-empty, meaningful)
- ‚úì Context empty on first request
- ‚úì Context contains correct number of prompts
- ‚úì Context sliding window (max 2 prompts)
- ‚úì Session persistence

### LLM Classification Quality
- ‚úì Intent classification correctness
- ‚úì Explanation relevance
- ‚úì Context summarization when >100 chars
- ‚úì Fallback behavior on errors

## Architecture

```
llm/planner/tests/
‚îú‚îÄ‚îÄ __init__.py           # Package initialization
‚îú‚îÄ‚îÄ runner.py             # Main test orchestrator
‚îú‚îÄ‚îÄ test_definitions.py   # 30 test case definitions
‚îú‚îÄ‚îÄ validator.py          # Validation logic
‚îú‚îÄ‚îÄ results/              # Test outputs (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ session_*.json
‚îÇ   ‚îî‚îÄ‚îÄ summary.json
‚îî‚îÄ‚îÄ README.md             # This file
```

### Key Components

**runner.py** - Test orchestrator
- Checks server health
- Runs 3 sessions in parallel using asyncio
- Executes tests sequentially within each session
- Aggregates results and generates reports
- Exit code 0 if all pass, 1 if any fail

**test_definitions.py** - Test case definitions
- Organizes 30 tests into 3 sessions
- Defines expected intents and behaviors
- Provides helper functions for test access

**validator.py** - Validation engine
- System behavior checks (structure, IDs, formats)
- LLM quality checks (classification, explanations)
- Session persistence validation
- Summary generation

## Edge Cases Covered

1. **Empty Context** - First request in session
2. **Partial Context** - Second request (only 1 previous)
3. **Full Context** - Third+ request (2 previous prompts)
4. **Context Truncation** - Only last 2 prompts retained
5. **Short Context** - No summarization triggered
6. **Long Context** - Summarization triggered at >100 chars
7. **Ambiguous Input** - "Fix that" ‚Üí clarify
8. **Very Short Input** - "Red" ‚Üí clarify
9. **Very Long Input** - Complex multi-component request
10. **Special Characters** - Arrows, percentages, symbols
11. **Technical Specs** - Numbers with units (px, %)
12. **Action Commands** - Scroll, click, navigate, hover
13. **Edit Requests** - Create, modify, style changes
14. **Step ID Uniqueness** - Verified across all requests

## Performance

- **Parallel Execution**: 3 sessions run simultaneously
- **Sequential Within Session**: Maintains context integrity
- **Rate Limit Compliance**: 30 tests in ~31 seconds (~0.97/sec)
- **K2 API Limit**: Stays under 30 requests/minute
- **Async HTTP**: httpx for non-blocking requests

## Interpreting Results

### Test Pass/Fail vs Classification Accuracy

**IMPORTANT**: These are two separate metrics!

#### ‚úÖ Test PASSES When:
**All SYSTEM CHECKS pass:**
- Response contains all required fields
- Step ID is valid UUID and unique
- Step type is valid (edit/act/clarify)
- Intent has proper format with explanation
- Context management is correct

#### üìä Classification Accuracy:
**Tracks whether LLM chose the expected intent type**
- Measured separately from test pass/fail
- Tests **don't fail** on classification mismatches
- LLM may have valid reasons to classify differently
- Reported as percentage (e.g., "86.7% accurate")

#### Example:
```
Input: "Change that"
Expected: clarify
Actual: edit

‚úÖ Test: PASS (all system checks OK)
‚ùå Classification: MISMATCH (tracked in accuracy)
```

### Viewing Misclassifications

#### Terminal Output:
Run tests to see detailed mismatch report:
```bash
./llm/venv/bin/python llm/planner/tests/runner.py
```

Output includes:
```
‚ö†Ô∏è  Classification Mismatches: 4

  [session-a] A8: Clarify intent - ambiguous reference
    Input:    "Change that"
    Expected: clarify
    Actual:   edit
```

#### Classification Report Files:
Each session has a detailed report:
```bash
cat results/session-a/classification_report.json
```

Shows `incorrect_classifications` array with all mismatches.

### Why This Design?

The LLM might have **valid reasons** to classify differently:
- Context awareness from previous prompts
- Reasonable alternative interpretation
- Pattern recognition we didn't anticipate

This design lets you:
- ‚úÖ Ensure system works (tests always validate structure)
- üìä Monitor LLM quality (classification tracked separately)
- üîç Review questionable decisions
- üéØ Adjust expectations if needed

**See `VALIDATION_LOGIC.md` for complete explanation.**

## Troubleshooting

### Server Not Running

```
‚ùå Error: Planner server not running at http://localhost:8000
```

**Solution**: Start the unified server first
```bash
source llm/venv/bin/activate
python -m uvicorn llm.server:app --host 0.0.0.0 --port 8000
```

**Important:** Make sure to use `llm.server:app` (not `llm.planner.server:app`) as the old planner-only server has been replaced by the unified server.

### Import Errors

```
ModuleNotFoundError: No module named 'httpx'
```

**Solution**: Install dependencies
```bash
source llm/venv/bin/activate
pip install httpx
```

### Request Timeouts

If tests timeout, check:
- K2 API key is valid in `llm/.env`
- Network connectivity
- K2 Think API status

## Development

### Adding New Tests

1. Edit `test_definitions.py`
2. Add test case to appropriate session
3. Define expected behavior
4. Run test suite

### Modifying Validations

1. Edit `validator.py`
2. Add new validation function
3. Call from `run_all_validations()`
4. Update assertions structure

### Extending Sessions

To add a 4th session:
1. Add to `TEST_SESSIONS` in `test_definitions.py`
2. Tests automatically run in parallel
3. Results auto-generated

## Notes

- Test results are **gitignored** (configured in `.gitignore`)
- Session files in `llm/planner/sessions/` are also gitignored
- Tests use **real K2 Think API** (counts against rate limit)
- Each test run creates fresh session files
- Previous test sessions are overwritten

## Last Test Run

**Date**: November 8, 2025  
**Results**: 30/30 passed (100.0%)  
**Classification Accuracy**: 96.7%  
**Duration**: 32.16 seconds  
**Status**: ‚úÖ All systems operational

**Improvements Made**:
- Updated test expectations for context-aware prompts (A8, A9, C6)
- Added "acceptable_intents" to allow valid alternative classifications
- Replaced vague test C15 with more concrete clarification test
- Implemented timestamped run folders for historical tracking 